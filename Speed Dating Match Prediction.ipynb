{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Formulation\n",
    "\n",
    "\n",
    "##### We want to find and predict the Speed Dating Match given the other features .\n",
    "\n",
    "##### Input :- Features collected from Surveys from partners.\n",
    "\n",
    "##### Output :- Predected Matching for partners .\n",
    "\n",
    "##### Data Mining Function :- Manipulating ,analyzing , preprocessing the data.\n",
    "\n",
    "#### Challenges  :\n",
    "\n",
    "##### Nan cells.\n",
    "##### Unused and unimportnat column.\n",
    "##### Convert the dtype.\n",
    "##### convert strings by One Hot encoding.\n",
    "##### Handling unbalanced data by over sampling\n",
    "#### Impact  : \n",
    "##### Predicting the match of the partners that will lead to a successful match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why a simple linear regression model (without any activation function) is not good for classification task, compared to Perceptron/Logistic regression? \n",
    "---------------------------------\n",
    "\n",
    "#### Simple linear regression models are not good for classification tasks because they don't provide a binary output, which is required for classification problems. Linear regression models predict continuous output values that can take on any value within a given range, while classification models require binary outputs, where the predicted value is either 0 or 1.\n",
    "\n",
    "#### Perceptron and logistic regression models, on the other hand, are designed specifically for binary classification tasks and use activation functions (such as the sigmoid function) to produce a binary output. They are able to learn decision boundaries that separate different classes and can handle non-linear relationships between input features and output labels. Therefore, they are more suitable for classification tasks compared to simple linear regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's a decision tree and how it is different to a logistic regression model?\n",
    "-----------------\n",
    "\n",
    "#### A decision tree is a type of supervised learning algorithm that is used for both classification and regression tasks. It is a tree-like model that uses a sequence of decisions on input features to arrive at a prediction. Each internal node in the tree represents a decision based on a feature, while each leaf node represents a class label or a numerical value.\n",
    "\n",
    "#### On the other hand, logistic regression is a type of linear regression that is used for binary classification tasks. It models the probability of the occurrence of an event using a logistic function, which maps any real-valued input to a value between 0 and 1. The decision boundary in logistic regression is a straight line, while in decision trees, it is a series of decisions on input features.\n",
    "\n",
    "#### The main difference between a decision tree and a logistic regression model is that decision trees are non-linear models and can handle non-linear relationships between input features and output labels, while logistic regression is a linear model that assumes a linear relationship between input features and output labels. Decision trees can also handle categorical and ordinal data, while logistic regression requires numerical input features. However, decision trees can be prone to overfitting, while logistic regression is less prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What's the difference between grid search and random search?\n",
    "--------------\n",
    "#### Grid search and random search are both hyperparameter optimization techniques used to find the best set of hyperparameters for a machine learning model. The main difference between them is the way they search the hyperparameter space.\n",
    "\n",
    "#### Grid search exhaustively searches over a predefined set of hyperparameters by creating a grid of all possible hyperparameter combinations. It evaluates each combination using cross-validation and returns the combination that produces the best performance.\n",
    "\n",
    "#### Random search, on the other hand, randomly samples hyperparameters from a predefined distribution. It searches over a larger hyperparameter space compared to grid search and can potentially find better hyperparameters. It also evaluates each combination using cross-validation and returns the combination that produces the best performance.\n",
    "\n",
    "#### Overall, grid search is a good option when the hyperparameter space is small and the possible combinations are easily enumerable, while random search is a good option when the hyperparameter space is large and the possible combinations are not easily enumerable. Random search is also more computationally efficient when the number of hyperparameters is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the difference between bayesian search and random search?\n",
    "-----\n",
    "\n",
    "#### Bayesian search and random search are both hyperparameter optimization techniques used to find the best set of hyperparameters for a machine learning model. The main difference between them is the way they search the hyperparameter space and update the search process.\n",
    "\n",
    "#### Random search randomly samples hyperparameters from a predefined distribution and evaluates each combination using cross-validation. It does not take into account the results of previous evaluations and is a purely random process.\n",
    "\n",
    "#### Bayesian search, on the other hand, uses the results of previous evaluations to update the search process. It models the distribution of the objective function (i.e., the performance metric of the model) as a probabilistic model and uses it to select the next set of hyperparameters to evaluate. The model updates its beliefs about the objective function as more evaluations are conducted and becomes more certain about the optimal hyperparameters over time.\n",
    "\n",
    "#### Overall, Bayesian search is a more intelligent approach than random search as it leverages the information gathered during the search process to guide the search towards promising regions of the hyperparameter space. This can lead to faster convergence and better performance compared to random search. However, Bayesian search requires more computational resources and is more complex to implement compared to random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing libraries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For uploading and accessing the data\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# for Encode categorical features as a numeric values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# for Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for fitting Models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>idg</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>partner</th>\n",
       "      <th>pid</th>\n",
       "      <th>...</th>\n",
       "      <th>sinc3_3</th>\n",
       "      <th>intel3_3</th>\n",
       "      <th>fun3_3</th>\n",
       "      <th>amb3_3</th>\n",
       "      <th>attr5_3</th>\n",
       "      <th>sinc5_3</th>\n",
       "      <th>intel5_3</th>\n",
       "      <th>fun5_3</th>\n",
       "      <th>amb5_3</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>372.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>63.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>331.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>357.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5904</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>214.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5905</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>199.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>290.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>151.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>542.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5909 rows × 192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  idg  condtn  wave  round  position  positin1  order  partner  \\\n",
       "0          0    3       2    14     18         2       2.0     14       12   \n",
       "1          1   14       1     3     10         2       NaN      8        8   \n",
       "2          1   14       1    13     10         8       8.0     10       10   \n",
       "3          1   38       2     9     20        18      13.0      6        7   \n",
       "4          1   24       2    14     20         6       6.0     20       17   \n",
       "...      ...  ...     ...   ...    ...       ...       ...    ...      ...   \n",
       "5904       0    1       2     9     20         2       2.0     18        1   \n",
       "5905       1   24       2     9     20        19      15.0      5        6   \n",
       "5906       0   13       2    11     21         5       5.0      3       18   \n",
       "5907       1   10       2     7     16         6      14.0      9       10   \n",
       "5908       0    7       2    21     22         7       7.0      2       12   \n",
       "\n",
       "        pid  ...  sinc3_3  intel3_3  fun3_3  amb3_3  attr5_3  sinc5_3  \\\n",
       "0     372.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "1      63.0  ...      8.0       8.0     7.0     8.0      NaN      NaN   \n",
       "2     331.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "3     200.0  ...      9.0       8.0     8.0     6.0      NaN      NaN   \n",
       "4     357.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "...     ...  ...      ...       ...     ...     ...      ...      ...   \n",
       "5904  214.0  ...     12.0      12.0     9.0    12.0      NaN      NaN   \n",
       "5905  199.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "5906  290.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "5907  151.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "5908  542.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "\n",
       "      intel5_3  fun5_3  amb5_3    id  \n",
       "0          NaN     NaN     NaN  2583  \n",
       "1          NaN     NaN     NaN  6830  \n",
       "2          NaN     NaN     NaN  4840  \n",
       "3          NaN     NaN     NaN  5508  \n",
       "4          NaN     NaN     NaN  4828  \n",
       "...        ...     ...     ...   ...  \n",
       "5904       NaN     NaN     NaN  3390  \n",
       "5905       NaN     NaN     NaN  4130  \n",
       "5906       NaN     NaN     NaN  1178  \n",
       "5907       NaN     NaN     NaN  5016  \n",
       "5908       NaN     NaN     NaN  8149  \n",
       "\n",
       "[5909 rows x 192 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the training dataset \n",
    "\n",
    "data= pd.read_csv(\"train.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>idg</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>partner</th>\n",
       "      <th>pid</th>\n",
       "      <th>...</th>\n",
       "      <th>sinc3_3</th>\n",
       "      <th>intel3_3</th>\n",
       "      <th>fun3_3</th>\n",
       "      <th>amb3_3</th>\n",
       "      <th>attr5_3</th>\n",
       "      <th>sinc5_3</th>\n",
       "      <th>intel5_3</th>\n",
       "      <th>fun5_3</th>\n",
       "      <th>amb5_3</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>368.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>212.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>162.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>407.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>215.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>513.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2469 rows × 191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  idg  condtn  wave  round  position  positin1  order  partner  \\\n",
       "0          0    5       2     2     16         3       NaN     13       13   \n",
       "1          0   33       2    14     18         6       6.0      4        8   \n",
       "2          1    6       2     9     20        10      16.0     15       19   \n",
       "3          1   26       2     2     19        15       NaN      8       10   \n",
       "4          0   29       2     7     16         7       7.0     10        5   \n",
       "...      ...  ...     ...   ...    ...       ...       ...    ...      ...   \n",
       "2464       0   23       2    15     19        18      18.0     14       11   \n",
       "2465       0    5       1    13      9         4       4.0      4        8   \n",
       "2466       1   26       2     2     19         3       NaN     15        3   \n",
       "2467       0   19       2     9     20        11      11.0      9        2   \n",
       "2468       1   38       2    21     22        22       7.0     16        5   \n",
       "\n",
       "        pid  ...  sinc3_3  intel3_3  fun3_3  amb3_3  attr5_3  sinc5_3  \\\n",
       "0      52.0  ...      7.0       8.0     6.0     8.0      NaN      NaN   \n",
       "1     368.0  ...      8.0       7.0     7.0     8.0      6.0      7.0   \n",
       "2     212.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "3      30.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "4     162.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "...     ...  ...      ...       ...     ...     ...      ...      ...   \n",
       "2464  407.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "2465  339.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "2466   23.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "2467  215.0  ...      7.0      12.0    12.0     9.0      NaN      NaN   \n",
       "2468  513.0  ...      9.0       8.0     7.0     8.0      5.0      8.0   \n",
       "\n",
       "      intel5_3  fun5_3  amb5_3    id  \n",
       "0          NaN     NaN     NaN   934  \n",
       "1          6.0     5.0     5.0  6539  \n",
       "2          NaN     NaN     NaN  6757  \n",
       "3          NaN     NaN     NaN  2275  \n",
       "4          NaN     NaN     NaN  1052  \n",
       "...        ...     ...     ...   ...  \n",
       "2464       NaN     NaN     NaN  7982  \n",
       "2465       NaN     NaN     NaN  7299  \n",
       "2466       NaN     NaN     NaN  1818  \n",
       "2467       NaN     NaN     NaN   937  \n",
       "2468       8.0     6.0     8.0  6691  \n",
       "\n",
       "[2469 rows x 191 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the training dataset \n",
    "\n",
    "data_test= pd.read_csv(\"test.csv\")\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5909, 192)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2469, 191)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_in_3    5449\n",
       "numdat_3    4849\n",
       "expnum      4627\n",
       "amb7_2      4519\n",
       "sinc7_2     4519\n",
       "            ... \n",
       "order          0\n",
       "partner        0\n",
       "match          0\n",
       "samerace       0\n",
       "gender         0\n",
       "Length: 192, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# anything missing?\n",
    "data.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5909 entries, 0 to 5908\n",
      "Columns: 192 entries, gender to id\n",
      "dtypes: float64(173), int64(11), object(8)\n",
      "memory usage: 8.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# checking types:\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for the columns which have more than 64 % null\n",
    "check = data.isnull().sum() / len(data) \n",
    "\n",
    "cols = check[check > 0.64].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['expnum',\n",
       " 'attr7_2',\n",
       " 'sinc7_2',\n",
       " 'intel7_2',\n",
       " 'fun7_2',\n",
       " 'amb7_2',\n",
       " 'shar7_2',\n",
       " 'numdat_3',\n",
       " 'num_in_3',\n",
       " 'attr7_3',\n",
       " 'sinc7_3',\n",
       " 'intel7_3',\n",
       " 'fun7_3',\n",
       " 'amb7_3',\n",
       " 'shar7_3',\n",
       " 'attr4_3',\n",
       " 'sinc4_3',\n",
       " 'intel4_3',\n",
       " 'fun4_3',\n",
       " 'amb4_3',\n",
       " 'shar4_3',\n",
       " 'attr2_3',\n",
       " 'sinc2_3',\n",
       " 'intel2_3',\n",
       " 'fun2_3',\n",
       " 'amb2_3',\n",
       " 'shar2_3',\n",
       " 'attr5_3',\n",
       " 'sinc5_3',\n",
       " 'intel5_3',\n",
       " 'fun5_3',\n",
       " 'amb5_3']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 22.30495853782366,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.13538669825689625,\n",
       " 0.0,\n",
       " 1.8446437637502116,\n",
       " 0.0,\n",
       " 1.100016923337282,\n",
       " 0.8123201895413776,\n",
       " 0.99847689964461,\n",
       " 0.99847689964461,\n",
       " 0.99847689964461,\n",
       " 1.116940260619394,\n",
       " 1.2354036215941784,\n",
       " 1.4046369944152985,\n",
       " 2.589270604163141,\n",
       " 3.536977491961415,\n",
       " 3.723134202064647,\n",
       " 4.484684379759689,\n",
       " 8.66474868844136,\n",
       " 13.31866644102217,\n",
       " 3.096970722626502,\n",
       " 3.9769842612963275,\n",
       " 4.653917752580809,\n",
       " 1.066170248773058,\n",
       " 0.7615501776950415,\n",
       " 0.99847689964461,\n",
       " 41.32678964291758,\n",
       " 62.17634117447961,\n",
       " 56.94702995430699,\n",
       " 0.7615501776950415,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 12.184802843120664,\n",
       " 48.43459130140464,\n",
       " 0.9815535623624978,\n",
       " 1.2184802843120663,\n",
       " 0.9815535623624978,\n",
       " 1.08309358605517,\n",
       " 1.6923337282112032,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 1.1846336097478423,\n",
       " 78.30428160433237,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 1.100016923337282,\n",
       " 1.2015569470299543,\n",
       " 1.4892536808258587,\n",
       " 22.778811981722797,\n",
       " 22.778811981722797,\n",
       " 22.778811981722797,\n",
       " 22.778811981722797,\n",
       " 22.778811981722797,\n",
       " 23.0665087155187,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 0.9815535623624978,\n",
       " 1.100016923337282,\n",
       " 1.100016923337282,\n",
       " 1.3707903198510747,\n",
       " 1.3707903198510747,\n",
       " 1.3707903198510747,\n",
       " 1.3707903198510747,\n",
       " 1.3707903198510747,\n",
       " 41.4960230157387,\n",
       " 41.4960230157387,\n",
       " 41.4960230157387,\n",
       " 41.4960230157387,\n",
       " 41.4960230157387,\n",
       " 2.420037231342021,\n",
       " 3.3846674564224064,\n",
       " 3.4692841428329664,\n",
       " 4.33237434422068,\n",
       " 8.529361990184464,\n",
       " 12.89558300896937,\n",
       " 2.9615840243696057,\n",
       " 3.8415975630394312,\n",
       " 4.264680995092232,\n",
       " 13.978676595024538,\n",
       " 50.668471822643426,\n",
       " 50.668471822643426,\n",
       " 50.668471822643426,\n",
       " 50.668471822643426,\n",
       " 50.668471822643426,\n",
       " 50.668471822643426,\n",
       " 51.93772211880183,\n",
       " 51.93772211880183,\n",
       " 51.93772211880183,\n",
       " 51.93772211880183,\n",
       " 51.93772211880183,\n",
       " 10.949399221526484,\n",
       " 10.949399221526484,\n",
       " 11.338635979015061,\n",
       " 76.12117109493992,\n",
       " 76.47656117786428,\n",
       " 76.12117109493992,\n",
       " 76.12117109493992,\n",
       " 76.47656117786428,\n",
       " 76.23963445591471,\n",
       " 11.135555931629717,\n",
       " 10.949399221526484,\n",
       " 10.949399221526484,\n",
       " 10.949399221526484,\n",
       " 10.949399221526484,\n",
       " 10.949399221526484,\n",
       " 31.24048062277881,\n",
       " 31.24048062277881,\n",
       " 31.24048062277881,\n",
       " 31.24048062277881,\n",
       " 31.24048062277881,\n",
       " 31.24048062277881,\n",
       " 31.24048062277881,\n",
       " 31.24048062277881,\n",
       " 31.24048062277881,\n",
       " 31.24048062277881,\n",
       " 31.24048062277881,\n",
       " 31.24048062277881,\n",
       " 10.949399221526484,\n",
       " 10.949399221526484,\n",
       " 10.949399221526484,\n",
       " 10.949399221526484,\n",
       " 10.949399221526484,\n",
       " 47.74073447283804,\n",
       " 47.74073447283804,\n",
       " 47.74073447283804,\n",
       " 47.74073447283804,\n",
       " 47.74073447283804,\n",
       " 52.54696226095786,\n",
       " 52.54696226095786,\n",
       " 52.54696226095786,\n",
       " 82.06126248096125,\n",
       " 92.21526485022846,\n",
       " 52.54696226095786,\n",
       " 52.54696226095786,\n",
       " 52.54696226095786,\n",
       " 52.54696226095786,\n",
       " 52.54696226095786,\n",
       " 52.54696226095786,\n",
       " 76.08732442037571,\n",
       " 76.08732442037571,\n",
       " 76.08732442037571,\n",
       " 76.08732442037571,\n",
       " 76.08732442037571,\n",
       " 76.08732442037571,\n",
       " 64.95176848874597,\n",
       " 64.95176848874597,\n",
       " 64.95176848874597,\n",
       " 64.95176848874597,\n",
       " 64.95176848874597,\n",
       " 64.95176848874597,\n",
       " 64.95176848874597,\n",
       " 64.95176848874597,\n",
       " 64.95176848874597,\n",
       " 64.95176848874597,\n",
       " 64.95176848874597,\n",
       " 76.08732442037571,\n",
       " 52.54696226095786,\n",
       " 52.54696226095786,\n",
       " 52.54696226095786,\n",
       " 52.54696226095786,\n",
       " 52.54696226095786,\n",
       " 76.08732442037571,\n",
       " 76.08732442037571,\n",
       " 76.08732442037571,\n",
       " 76.08732442037571,\n",
       " 76.08732442037571,\n",
       " 0.0]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Below code gives percentage of null in every column\n",
    "null_percentage = data.isnull().sum()/data.shape[0]*100\n",
    "list(null_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['expnum',\n",
       " 'attr7_2',\n",
       " 'sinc7_2',\n",
       " 'intel7_2',\n",
       " 'fun7_2',\n",
       " 'amb7_2',\n",
       " 'shar7_2',\n",
       " 'numdat_3',\n",
       " 'num_in_3',\n",
       " 'attr7_3',\n",
       " 'sinc7_3',\n",
       " 'intel7_3',\n",
       " 'fun7_3',\n",
       " 'amb7_3',\n",
       " 'shar7_3',\n",
       " 'attr4_3',\n",
       " 'sinc4_3',\n",
       " 'intel4_3',\n",
       " 'fun4_3',\n",
       " 'amb4_3',\n",
       " 'shar4_3',\n",
       " 'attr2_3',\n",
       " 'sinc2_3',\n",
       " 'intel2_3',\n",
       " 'fun2_3',\n",
       " 'amb2_3',\n",
       " 'shar2_3',\n",
       " 'attr5_3',\n",
       " 'sinc5_3',\n",
       " 'intel5_3',\n",
       " 'fun5_3',\n",
       " 'amb5_3']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Below code gives list of columns having more than 64% null\n",
    "col_to_drop = null_percentage[null_percentage>64].keys()\n",
    "\n",
    "data = data.drop(col_to_drop, axis=1)\n",
    "list(col_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['expnum',\n",
       " 'attr7_2',\n",
       " 'sinc7_2',\n",
       " 'intel7_2',\n",
       " 'fun7_2',\n",
       " 'amb7_2',\n",
       " 'shar7_2',\n",
       " 'numdat_3',\n",
       " 'num_in_3',\n",
       " 'attr7_3',\n",
       " 'sinc7_3',\n",
       " 'intel7_3',\n",
       " 'fun7_3',\n",
       " 'amb7_3',\n",
       " 'shar7_3',\n",
       " 'attr4_3',\n",
       " 'sinc4_3',\n",
       " 'intel4_3',\n",
       " 'fun4_3',\n",
       " 'amb4_3',\n",
       " 'shar4_3',\n",
       " 'attr2_3',\n",
       " 'sinc2_3',\n",
       " 'intel2_3',\n",
       " 'fun2_3',\n",
       " 'amb2_3',\n",
       " 'shar2_3',\n",
       " 'attr5_3',\n",
       " 'sinc5_3',\n",
       " 'intel5_3',\n",
       " 'fun5_3',\n",
       " 'amb5_3']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Below code gives percentage of null in every column\n",
    "null_percentage = data_test.isnull().sum()/data_test.shape[0]*100\n",
    "\n",
    "# Below code gives list of columns having more than 64% null\n",
    "col_to_drop = null_percentage[null_percentage>64].keys()\n",
    "\n",
    "data_test = data_test.drop(col_to_drop, axis=1)\n",
    "list(col_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5909, 160)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2469, 159)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['field',\n",
       " 'undergra',\n",
       " 'mn_sat',\n",
       " 'tuition',\n",
       " 'from',\n",
       " 'zipcode',\n",
       " 'income',\n",
       " 'career']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the non-numeric columns from training data\n",
    "non_numeric_cols = list(data.select_dtypes(exclude=['number']).columns)\n",
    "list(non_numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['field', 'undergra', 'from', 'zipcode', 'career']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the non-numeric columns from test data\n",
    "\n",
    "non_numeric_cols_test = list(data_test.select_dtypes(exclude=['number']).columns)\n",
    "list(non_numeric_cols_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert it to categories types \n",
    "for i in non_numeric_cols:\n",
    "    data[i] = data[i].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same for testing data\n",
    "for i in non_numeric_cols_test:\n",
    "    data_test[i] = data_test[i].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5909 entries, 0 to 5908\n",
      "Columns: 160 entries, gender to id\n",
      "dtypes: category(8), float64(141), int64(11)\n",
      "memory usage: 7.0 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "5904    0\n",
       "5905    0\n",
       "5906    0\n",
       "5907    1\n",
       "5908    0\n",
       "Name: match, Length: 5909, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['match']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>idg</th>\n",
       "      <th>condtn</th>\n",
       "      <th>wave</th>\n",
       "      <th>round</th>\n",
       "      <th>position</th>\n",
       "      <th>positin1</th>\n",
       "      <th>order</th>\n",
       "      <th>partner</th>\n",
       "      <th>pid</th>\n",
       "      <th>...</th>\n",
       "      <th>sinc1_3</th>\n",
       "      <th>intel1_3</th>\n",
       "      <th>fun1_3</th>\n",
       "      <th>amb1_3</th>\n",
       "      <th>shar1_3</th>\n",
       "      <th>attr3_3</th>\n",
       "      <th>sinc3_3</th>\n",
       "      <th>intel3_3</th>\n",
       "      <th>fun3_3</th>\n",
       "      <th>amb3_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>372.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>63.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>331.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.37</td>\n",
       "      <td>18.37</td>\n",
       "      <td>18.37</td>\n",
       "      <td>14.29</td>\n",
       "      <td>14.29</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>357.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5904</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>214.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.87</td>\n",
       "      <td>18.87</td>\n",
       "      <td>15.09</td>\n",
       "      <td>16.98</td>\n",
       "      <td>13.21</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5905</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>199.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>290.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>151.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>542.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5909 rows × 159 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender  idg  condtn  wave  round  position  positin1  order  partner  \\\n",
       "0          0    3       2    14     18         2       2.0     14       12   \n",
       "1          1   14       1     3     10         2       NaN      8        8   \n",
       "2          1   14       1    13     10         8       8.0     10       10   \n",
       "3          1   38       2     9     20        18      13.0      6        7   \n",
       "4          1   24       2    14     20         6       6.0     20       17   \n",
       "...      ...  ...     ...   ...    ...       ...       ...    ...      ...   \n",
       "5904       0    1       2     9     20         2       2.0     18        1   \n",
       "5905       1   24       2     9     20        19      15.0      5        6   \n",
       "5906       0   13       2    11     21         5       5.0      3       18   \n",
       "5907       1   10       2     7     16         6      14.0      9       10   \n",
       "5908       0    7       2    21     22         7       7.0      2       12   \n",
       "\n",
       "        pid  ...  sinc1_3  intel1_3  fun1_3  amb1_3  shar1_3  attr3_3  \\\n",
       "0     372.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "1      63.0  ...    20.00     15.00   20.00   10.00    15.00      6.0   \n",
       "2     331.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "3     200.0  ...    18.37     18.37   18.37   14.29    14.29      8.0   \n",
       "4     357.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "...     ...  ...      ...       ...     ...     ...      ...      ...   \n",
       "5904  214.0  ...    18.87     18.87   15.09   16.98    13.21     12.0   \n",
       "5905  199.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "5906  290.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "5907  151.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "5908  542.0  ...      NaN       NaN     NaN     NaN      NaN      NaN   \n",
       "\n",
       "      sinc3_3  intel3_3  fun3_3  amb3_3  \n",
       "0         NaN       NaN     NaN     NaN  \n",
       "1         8.0       8.0     7.0     8.0  \n",
       "2         NaN       NaN     NaN     NaN  \n",
       "3         9.0       8.0     8.0     6.0  \n",
       "4         NaN       NaN     NaN     NaN  \n",
       "...       ...       ...     ...     ...  \n",
       "5904     12.0      12.0     9.0    12.0  \n",
       "5905      NaN       NaN     NaN     NaN  \n",
       "5906      NaN       NaN     NaN     NaN  \n",
       "5907      NaN       NaN     NaN     NaN  \n",
       "5908      NaN       NaN     NaN     NaN  \n",
       "\n",
       "[5909 rows x 159 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#droping id cloumn \n",
    "data.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape (5909, 159) (5909,)\n"
     ]
    }
   ],
   "source": [
    "# now we can split the data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data['match'] # lower case for vector\n",
    "X = data.drop('match', axis=1) # upper case for matrix\n",
    "print('original shape', X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4921\n",
       "1     988\n",
       "Name: match, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.match.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric features: ['gender', 'idg', 'condtn', 'wave', 'round', 'position', 'positin1', 'order', 'partner', 'pid', 'int_corr', 'samerace', 'age_o', 'race_o', 'pf_o_att', 'pf_o_sin', 'pf_o_int', 'pf_o_fun', 'pf_o_amb', 'pf_o_sha', 'attr_o', 'sinc_o', 'intel_o', 'fun_o', 'amb_o', 'shar_o', 'like_o', 'prob_o', 'met_o', 'age', 'field_cd', 'race', 'imprace', 'imprelig', 'goal', 'date', 'go_out', 'career_c', 'sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga', 'exphappy', 'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1', 'attr4_1', 'sinc4_1', 'intel4_1', 'fun4_1', 'amb4_1', 'shar4_1', 'attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1', 'attr3_1', 'sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1', 'attr5_1', 'sinc5_1', 'intel5_1', 'fun5_1', 'amb5_1', 'attr', 'sinc', 'intel', 'fun', 'amb', 'shar', 'like', 'prob', 'met', 'match_es', 'attr1_s', 'sinc1_s', 'intel1_s', 'fun1_s', 'amb1_s', 'shar1_s', 'attr3_s', 'sinc3_s', 'intel3_s', 'fun3_s', 'amb3_s', 'satis_2', 'length', 'numdat_2', 'attr1_2', 'sinc1_2', 'intel1_2', 'fun1_2', 'amb1_2', 'shar1_2', 'attr4_2', 'sinc4_2', 'intel4_2', 'fun4_2', 'amb4_2', 'shar4_2', 'attr2_2', 'sinc2_2', 'intel2_2', 'fun2_2', 'amb2_2', 'shar2_2', 'attr3_2', 'sinc3_2', 'intel3_2', 'fun3_2', 'amb3_2', 'attr5_2', 'sinc5_2', 'intel5_2', 'fun5_2', 'amb5_2', 'you_call', 'them_cal', 'date_3', 'attr1_3', 'sinc1_3', 'intel1_3', 'fun1_3', 'amb1_3', 'shar1_3', 'attr3_3', 'sinc3_3', 'intel3_3', 'fun3_3', 'amb3_3', 'id']\n",
      "categorical features: ['field', 'undergra', 'mn_sat', 'tuition', 'from', 'zipcode', 'income', 'career']\n"
     ]
    }
   ],
   "source": [
    "# we extract numeric features and categorical features names\n",
    "# for later use\n",
    "\n",
    "# numeric features can be selected by: (based on the df2.info() output )\n",
    "features_numeric = list(X.select_dtypes(include=['float64', 'int64']))\n",
    "\n",
    "# categorical features can be selected by: (based on the df2.info() output )\n",
    "features_categorical = list(X.select_dtypes(include=['category']))\n",
    "\n",
    "print('numeric features:', features_numeric)\n",
    "print('categorical features:', features_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\programdata\\anaconda3\\lib\\site-packages (1.7.4)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.19.2)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('num',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['gender', 'idg', 'condtn',\n",
       "                                                   'wave', 'round', 'position',\n",
       "                                                   'positin1', 'order',\n",
       "                                                   'partner', 'pid', 'int_corr',\n",
       "                                                   'samerace', 'age_o',\n",
       "                                                   'race_o', 'pf_o_att',\n",
       "                                                   'pf_o_sin', 'pf_o_int',\n",
       "                                                   'pf_o_fun', 'pf_o_amb',\n",
       "                                                   'pf_o_sha', 'attr_o',\n",
       "                                                   'sinc_o', 'intel_o', 'fun_o',\n",
       "                                                   'amb_o', 'shar_o', 'like_o',\n",
       "                                                   'prob_o', 'met_o', 'age', ...]),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('imputer',\n",
       "                                                                   SimpleImputer(strategy='constant')),\n",
       "                                                                  ('onehot',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['field', 'undergra',\n",
       "                                                   'mn_sat', 'tuition', 'from',\n",
       "                                                   'zipcode', 'income',\n",
       "                                                   'career'])])),\n",
       "                ('my_classifier', SVC())])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import  GridSearchCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# define a pipe line for numeric feature preprocessing\n",
    "# we gave them a name so we can set their hyperparameters\n",
    "transformer_numeric = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer()),\n",
    "        ('scaler', StandardScaler())]\n",
    ")\n",
    "\n",
    "# define a pipe line for categorical feature preprocessing\n",
    "# we gave them a name so we can set their hyperparameters\n",
    "transformer_categorical = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ]\n",
    ")\n",
    "# define the preprocessor \n",
    "# we gave them a name so we can set their hyperparameters\n",
    "# we also specify what are the categorical \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', transformer_numeric, features_numeric),\n",
    "        ('cat', transformer_categorical, features_categorical)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# combine the preprocessor with the model as a full tunable pipeline\n",
    "# we gave them a name so we can set their hyperparameters\n",
    "full_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classifier', svm.SVC())\n",
    "    ]\n",
    ")\n",
    "\n",
    "full_pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search with Cross-validation\n",
    "\n",
    "### First Trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=2)]: Done   3 out of   5 | elapsed:  5.2min remaining:  3.5min\n",
      "[Parallel(n_jobs=2)]: Done   5 out of   5 | elapsed:  7.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   5 out of   5 | elapsed:  7.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8658585766087743\n",
      "best score {'my_classifier__class_weight': 'balanced', 'my_classifier__max_depth': 20, 'my_classifier__max_features': None, 'my_classifier__min_samples_leaf': 2, 'my_classifier__min_samples_split': 2, 'my_classifier__n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# here we specify the search space\n",
    "# `__` denotes an attribute of the preceeding name\n",
    "# (e.g. my_classifier__n_estimators means the `n_estimators` param for `my_classifier`)\n",
    "\n",
    "#trying with first Classifier RandomForest\n",
    "full_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classifier', RandomForestClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# these parameters which I had got from a lot of many trials give me this best one\n",
    "\n",
    "param_grid = {'my_classifier__class_weight': ['balanced'],\n",
    "              'my_classifier__max_depth': [20],\n",
    "              'my_classifier__max_features': [None],\n",
    "              'my_classifier__min_samples_leaf': [2], \n",
    "              'my_classifier__min_samples_split': [2], \n",
    "              'my_classifier__n_estimators': [300]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# cv=5 means five-fold cross-validation\n",
    "# n_jobs means the cucurrent number of jobs\n",
    "\n",
    "#here we define our gridSearch and passing the pipline as a pramater and our grid search \n",
    "grid_search = GridSearchCV(\n",
    "    full_pipline, param_grid, cv=5, verbose=10, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print('best score {}'.format(grid_search.best_score_))\n",
    "print('best score {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5909, 160)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict with test data and save the result\n",
    "y_pred1= grid_search.predict_proba(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92025542, 0.07974458],\n",
       "       [0.69455978, 0.30544022],\n",
       "       [0.76038607, 0.23961393],\n",
       "       ...,\n",
       "       [0.92510097, 0.07489903],\n",
       "       [0.93582531, 0.06417469],\n",
       "       [0.95271081, 0.04728919]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#saving the output on csv file for submission\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = data_test['id']\n",
    "\n",
    "submission['match'] = y_pred1[:,1]\n",
    "\n",
    "submission.to_csv('sample_submission_Rf.csv' , index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "### Second Trail "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   26.9s\n",
      "[Parallel(n_jobs=2)]: Done   4 tasks      | elapsed:   54.7s\n",
      "[Parallel(n_jobs=2)]: Done   9 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=2)]: Done  14 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=2)]: Done  21 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=2)]: Done  37 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=2)]: Done  50 out of  50 | elapsed: 10.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8659384934494712\n",
      "best score {'my_classifier__kernel': 'sigmoid', 'my_classifier__gamma': 'auto', 'my_classifier__degree': 4, 'my_classifier__class_weight': 'balanced', 'my_classifier__C': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "#then in trail 2 we trying Random Search with SVC model\n",
    "\n",
    "full_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classifier', svm.SVC(probability=True))\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "\n",
    "    'my_classifier__C': [0.1, 1, 10],\n",
    "    'my_classifier__kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'my_classifier__degree': [2, 3, 4],\n",
    "    'my_classifier__gamma': ['scale', 'auto'],\n",
    "    'my_classifier__class_weight': [None, 'balanced']\n",
    "   \n",
    "} \n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    full_pipline, param_grid, cv=5, verbose=10, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "random_search.fit(X, y)\n",
    "\n",
    "print('best score {}'.format(random_search.best_score_))\n",
    "print('best score {}'.format(random_search.best_params_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86456271, 0.13543729],\n",
       "       [0.62426008, 0.37573992],\n",
       "       [0.73647826, 0.26352174],\n",
       "       ...,\n",
       "       [0.92663196, 0.07336804],\n",
       "       [0.96726241, 0.03273759],\n",
       "       [0.93970406, 0.06029594]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##predict with test data and save the result\n",
    "\n",
    "y_pred2= random_search.predict_proba(data_test)\n",
    "y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the output on csv file for submission\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = data_test['id']\n",
    "\n",
    "submission['match'] = y_pred2[:,1]\n",
    "\n",
    "submission.to_csv('sample_submission_SVM.csv' , index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:278: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=2)]: Done   3 out of   5 | elapsed:  4.7min remaining:  3.2min\n",
      "[Parallel(n_jobs=2)]: Done   5 out of   5 | elapsed:  7.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   5 out of   5 | elapsed:  7.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8668970498513879\n",
      "best score {'my_classifier__n_estimators': 300, 'my_classifier__min_samples_split': 2, 'my_classifier__min_samples_leaf': 2, 'my_classifier__max_features': None, 'my_classifier__max_depth': 20, 'my_classifier__class_weight': 'balanced'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "#in third trail we trying to get better scrore so we tried XGB Classifier with Random Search \n",
    "XGB_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classfier', XGBClassifier())\n",
    "    ]\n",
    ")\n",
    "# these parameters which I had got from a lot of many trials give me this best one\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'my_classfier__learning_rate': [0.1],\n",
    "    'my_classfier__max_depth': [10],\n",
    "    'my_classfier__n_estimators': [500],\n",
    "    'my_classfier__subsample': [0.9],\n",
    "    'my_classfier__colsample_bytree': [0.5, 0.7, 0.9],\n",
    "    'my_classfier__reg_alpha': [0.1],\n",
    "    'my_classfier__reg_lambda': [0.1],\n",
    "    'my_classfier__gamma': [0]\n",
    "}\n",
    "\n",
    "\n",
    "rand2_search = RandomizedSearchCV(\n",
    "    XGB_pipline, param_grid, cv=5, verbose=10, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "rand2_search.fit(X, y)\n",
    "\n",
    "print('best score {}'.format(rand2_search.best_score_))\n",
    "print('best score {}'.format(rand2_search.best_params_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9791086e-01, 2.0891202e-03],\n",
       "       [2.1614575e-01, 7.8385425e-01],\n",
       "       [9.5386559e-01, 4.6134405e-02],\n",
       "       ...,\n",
       "       [9.8932838e-01, 1.0671640e-02],\n",
       "       [9.9831152e-01, 1.6885012e-03],\n",
       "       [9.9928391e-01, 7.1607879e-04]], dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict with test data and save the result\n",
    "y_pred3= rand2_search.predict_proba(data_test)\n",
    "y_pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the output on csv file for submission\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = data_test['id']\n",
    "\n",
    "submission['match'] = y_pred3[:,1]\n",
    "\n",
    "submission.to_csv('sample_submission_XGB1.csv' , index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in third Trail i got best result for now XGBoost Classifier with Random Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   1 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=2)]: Done   3 out of   5 | elapsed:   48.0s remaining:   32.0s\n",
      "[Parallel(n_jobs=2)]: Done   5 out of   5 | elapsed:  1.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   5 out of   5 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.883136666986805\n",
      "best score {'my_classfier__colsample_bytree': 1.0, 'my_classfier__gamma': 0.5, 'my_classfier__learning_rate': 0.1, 'my_classfier__max_depth': 10, 'my_classfier__n_estimators': 900, 'my_classfier__reg_alpha': 0.5, 'my_classfier__reg_lambda': 2, 'my_classfier__subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "XGB_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classfier', XGBClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# these parameters which I had got from a lot of many trials give me this best one\n",
    "\n",
    "param_grid = {\n",
    "    \n",
    "'my_classfier__colsample_bytree': [1.0],\n",
    "    'my_classfier__gamma': [0.5], \n",
    "    'my_classfier__learning_rate': [0.1],\n",
    "    'my_classfier__max_depth': [10], \n",
    "    'my_classfier__n_estimators':[ 900],\n",
    "    'my_classfier__reg_alpha': [0.5],\n",
    "    'my_classfier__reg_lambda': [2], \n",
    "    'my_classfier__subsample': [0.7]\n",
    "}\n",
    "\n",
    "\n",
    "grid2_search = GridSearchCV(\n",
    "    XGB_pipline, param_grid, cv=5, verbose=10, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "grid2_search.fit(X, y)\n",
    "\n",
    "print('best score {}'.format(grid2_search.best_score_))\n",
    "print('best score {}'.format(grid2_search.best_params_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9809214 , 0.01907861],\n",
       "       [0.4473244 , 0.5526756 ],\n",
       "       [0.8476882 , 0.15231179],\n",
       "       ...,\n",
       "       [0.9422367 , 0.0577633 ],\n",
       "       [0.99369574, 0.00630425],\n",
       "       [0.99593085, 0.00406914]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred4= grid2_search.predict_proba(data_test)\n",
    "y_pred4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = data_test['id']\n",
    "\n",
    "submission['match'] = y_pred4[:,1]\n",
    "\n",
    "submission.to_csv('sample_submission_XGB_bayes.csv' , index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   5 out of   5 | elapsed:   24.0s finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   5 out of   5 | elapsed:   20.5s finished\n",
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   5 out of   5 | elapsed:   25.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8830035640970934\n",
      "best score OrderedDict([('my_classfier__colsample_bytree', 0.7), ('my_classfier__gamma', 0), ('my_classfier__learning_rate', 0.1), ('my_classfier__max_depth', 10), ('my_classfier__n_estimators', 500), ('my_classfier__reg_alpha', 0.1), ('my_classfier__reg_lambda', 0.1), ('my_classfier__subsample', 0.9)])\n"
     ]
    }
   ],
   "source": [
    "# Let's try this with xgb model with bayes Search \n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "full_pipline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('my_classfier', XGBClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# these parameters which I had got from a lot of many trials give me this best one\n",
    "\n",
    "param_grid = {\n",
    "    'my_classfier__learning_rate': [0.1],\n",
    "    'my_classfier__max_depth': [10],\n",
    "    'my_classfier__n_estimators': [500],\n",
    "    'my_classfier__subsample': [0.9],\n",
    "    'my_classfier__colsample_bytree': [0.5, 0.7, 0.9],\n",
    "    'my_classfier__reg_alpha': [0.1],\n",
    "    'my_classfier__reg_lambda': [0.1],\n",
    "    'my_classfier__gamma': [0]\n",
    "}\n",
    "\n",
    "\n",
    "bayes_search = BayesSearchCV(full_pipline,param_grid, cv=5, verbose=1, n_jobs=2, n_iter=3,scoring='roc_auc')\n",
    "\n",
    "bayes_search.fit(X,y)\n",
    "\n",
    "print('best score {}'.format(bayes_search.best_score_))\n",
    "\n",
    "print('best score {}'.format(bayes_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred5= bayes_search.predict_proba(data_test)\n",
    "y_pred5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally i got the best score with XGBoost with Random Search \n",
    "### 0.8668970498513879\n",
    "### with these hayperparmaters\n",
    "----\n",
    "###### {'my_classifier__n_estimators': 300, 'my_classifier__min_samples_split': 2, 'my_classifier__min_samples_leaf': 2, 'my_classifier__max_features': None, 'my_classifier__max_depth': 20, 'my_classifier__class_weight': 'balanced'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
